import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import ReflectionPad2d
from torch.nn import ZeroPad2d
from torch.autograd import Function
import numpy as np


def iscomplex(input):
    return input.size(-1) == 2


def ones_like(z):
    re = torch.ones_like(z[..., 0])
    im = torch.zeros_like(z[..., 1])
    return torch.stack((re, im), dim=-1)

def real(z):
    return z[..., 0]


def imag(z):
    return z[..., 1]


def conjugate(z):
    z_copy = z.clone()
    z_copy[..., 1] = -z_copy[..., 1]
    return z_copy


def pows(z, max_k, dim=0):
    z_pows = [ones_like(z)]
    if max_k > 0:
        z_pows.append(z)
        z_acc = z
        for k in range(2, max_k + 1):
            z_acc = mul(z_acc, z)
            z_pows.append(z_acc)
    z_pows = torch.stack(z_pows, dim=dim)
    return z_pows


def log2_pows(z, max_pow_k, dim=0):
    z_pows = [ones_like(z)]
    if max_pow_k > 0:
        z_pows.append(z)
        z_acc = z
        for k in range(2, max_pow_k + 1):
            z_acc = mul(z_acc, z_acc)
            z_pows.append(z_acc)
    assert len(z_pows) == max_pow_k + 1
    z_pows = torch.stack(z_pows, dim=dim)
    return z_pows

def mul(z1, z2):
    zr = real(z1) * real(z2) - imag(z1) * imag(z2)
    zi = real(z1) * imag(z2) + imag(z1) * real(z2)
    z = torch.stack((zr, zi), dim=-1)
    return z

# substract spatial mean (complex valued input)
class SubInitSpatialMeanC(object):
    def __init__(self):
        self.minput = None

    def __call__(self, input):
        if self.minput is None:
            minput = input.clone().detach()
            minput = torch.mean(minput, -2, True)
            minput = torch.mean(minput, -3, True)
            self.minput = minput
            print('sum of minput',self.minput.sum())

        output = input - self.minput
        return output

# substract spatial mean (real valued input)
class SubInitSpatialMeanR(object):
    def __init__(self):
        self.minput = None

    def __call__(self, input):
        if self.minput is None:
            minput = input.clone().detach()
            minput = torch.mean(minput, -1, True)
            minput = torch.mean(minput, -2, True)
            self.minput = minput
            print('sum of minput',self.minput.sum())

        output = input - self.minput
        return output

class SubInitMeanIso(object):
    def __init__(self):
        self.minput = None

    def __call__(self, input):
        if self.minput is None:
            minput = input.clone().detach()  # input size:(J,Q,K,M,N,2)
            minput = torch.mean(minput, -2, True)
            minput = torch.mean(minput, -3, True)
            minput[:, 1:, ...] = 0
            self.minput = minput
            print('sum of minput', self.minput.sum())
            # print('minput shape is', self.minput.shape)
        output = input - self.minput
        return output

class DivInitStd(object):
    def __init__(self,stdcut=0):
        self.stdinput = None
        self.eps = stdcut
        print('DivInitStd:stdcut',stdcut)
    
    def __call__(self, input):
        if self.stdinput is None:
            stdinput = input.clone().detach()  # input size:(J,Q,K,M,N,2)
            # m = torch.mean(torch.mean(stdinput, -2, True), -3, True)
            # stdinput = stdinput - m
            d = input.shape[-2]*input.shape[-3] 
            stdinput = torch.norm(stdinput, dim=-1, keepdim=True)
            stdinput = torch.norm(stdinput, dim=(-2, -3), keepdim=True)
            self.stdinput = stdinput  / np.sqrt(d)
            self.stdinput = self.stdinput + self.eps
            print('stdinput max,min:',self.stdinput.max(),self.stdinput.min())

        output = input/self.stdinput
        return output

class DivInitStdQ0(object):
    def __init__(self):
        self.stdinput = None
        self.eps = 1e-16
        
    def __call__(self, input):
        if self.stdinput is None:
            stdinput = input.clone().detach()  # input size:(J,Q,K,M,N,2)
            stdinput = stdinput[:, 0, ...].unsqueeze(1)  # size:(J,1,K,M,N,2)
            d = input.shape[-2]*input.shape[-3] 
            stdinput = torch.norm(stdinput, dim=-1, keepdim=True)
            stdinput = torch.norm(stdinput, dim=(-2, -3), keepdim=True) / np.sqrt(d)
            self.stdinput = stdinput + self.eps
            print('stdinput max,min:',self.stdinput.max(),self.stdinput.min())
        output = input/self.stdinput
        return output

# substract spatial mean (complex valued input), average over ell
class SubInitSpatialMeanCL(object):
    def __init__(self):
        self.minput = None

    def __call__(self, input): # input: (J,L2,M,N,K,2)
        if self.minput is None:
            minput = input.clone().detach()
            minput = torch.mean(minput, dim=1, keepdim=True)
            minput = torch.mean(minput, dim=2, keepdim=True)
            minput = torch.mean(minput, dim=3, keepdim=True)
            self.minput = minput # .expand_as(input)
            print('minput size',self.minput.shape)
            print('sum of minput',self.minput.sum())

        output = input - self.minput
        return output
    
# divide by std, average over ell
class DivInitStdL(object):
    def __init__(self):
        self.stdinput = None

    def __call__(self, input): # input: (J,L2,M,N,K,2)
        if self.stdinput is None:
            stdinput = input.clone().detach()
            #dl = input.shape[1]*input.shape[2]*input.shape[3] 
            stdinput = torch.norm(stdinput, dim=-1, keepdim=True) # (J,L2,M,N,K,1)
            stdinput = torch.mul(stdinput,stdinput)
            stdinput = torch.mean(stdinput, dim=1, keepdim=True)
            stdinput = torch.mean(stdinput, dim=2, keepdim=True)
            stdinput = torch.mean(stdinput, dim=3, keepdim=True)
            self.stdinput = torch.sqrt(stdinput) # .expand_as(input) #  / dl)
            print('stdinput size',self.stdinput.shape)
            print('stdinput max,min:',self.stdinput.max(),self.stdinput.min())
        
        output = input/self.stdinput
        return output

class SubInitSpatialMeanCinFFT(object):
    def __init__(self):
        self.minput = None

    def __call__(self, input):
        if self.minput is None:
            minput = input.clone().detach()
            minput = input[...,0,0,:] # zero-freq. value torch.mean(minput, -2, True)
            #minput = torch.mean(minput, -3, True)
            self.minput = minput
            print('sum of minput',self.minput.sum())

        output = input
        output[...,0,0,:] = input[...,0,0,:] - self.minput
        return output

class SubsampleFourier(object):
    """
        Subsampling of a 2D image performed in the Fourier domain
        Subsampling in the spatial domain amounts to periodization
        in the Fourier domain, hence the formula.
        Parameters
        ----------
        x : tensor_like
            input tensor with at least 5 dimensions, the last being the real
             and imaginary parts.
            Ideally, the last dimension should be a power of 2 to avoid errors.
        k : int
            integer such that x is subsampled by 2**k along the spatial variables.
        Returns
        -------
        res : tensor_like
            tensor such that its fourier transform is the Fourier
            transform of a subsampled version of x, i.e. in
            FFT^{-1}(res)[u1, u2] = FFT^{-1}(x)[u1 * (2**k), u2 * (2**k)]
    """
    def __call__(self, input, k):
        out = input.new(input.size(0), input.size(1), input.size(2) // k, input.size(3) // k, 2)


        y = input.view(input.size(0), input.size(1),
                       input.size(2)//out.size(2), out.size(2),
                       input.size(3)//out.size(3), out.size(3),
                       2)

        out = y.mean(4, keepdim=False).mean(2, keepdim=False)
        return out

class SubInitMean(object):
    def __init__(self, dim):
        self.dim = dim # use the last "dim" dimensions to compute the mean
        self.minput = None

    def __call__(self, input):
        if self.minput is None:
            minput = input.clone().detach()
            #print('subinitmean:input',input.shape)
            for d in range(self.dim):
                minput = torch.mean(minput, -1)
            for d in range(self.dim):
                minput = minput.unsqueeze(-1)
            #print('subinitmean:minput',minput.shape)
            minput.expand_as(input)
            self.minput = minput

        #print('subinitmean:minput sum',self.minput.sum())
        output = input - self.minput
        return output

class Pad(object):
    def __init__(self, pad_size, pre_pad=False, pad_mode='zero'):
        """
            Padding which allows to simultaneously pad in a reflection fashion
            and map to complex.
            Parameters
            ----------
            pad_size : int
                size of padding to apply.
            pre_pad : boolean
                if set to true, then there is no padding, one simply adds the imaginarty part.
        """
        self.pre_pad = pre_pad
        if pad_mode == 'Reflect':
            print('use reflect pad')
            self.padding_module = ReflectionPad2d(pad_size)
        else:
            print('use zero pad')
            self.padding_module = ZeroPad2d(pad_size)

    def __call__(self, input):
        if(self.pre_pad):
            output = input.new_zeros(input.size(0), input.size(1), input.size(2), input.size(3), 2)
            output.narrow(output.ndimension()-1, 0, 1)[:] = input
        else:
            out_ = self.padding_module(input)
            output = input.new_zeros(*(out_.size() + (2,)))
            output.select(4, 0)[:] = out_
        return output

def unpad(in_):
    """
        Slices the input tensor at indices between 1::-1
        Parameters
        ----------
        in_ : tensor_like
            input tensor
        Returns
        -------
        in_[..., 1:-1, 1:-1]
    """
    return in_[..., 1:-1, 1:-1]

class Modulus(object):
    """
        This class implements a modulus transform for complex numbers.
        Usage
        -----
        modulus = Modulus()
        x_mod = modulus(x)
        Parameters
        ---------
        x: input tensor, with last dimension = 2 for complex numbers
        Returns
        -------
        output: a tensor with imaginary part set to 0, real part set equal to
        the modulus of x.
    """
    def __call__(self, input):

        norm = input.norm(p=2, dim=-1, keepdim=True)
        return torch.cat([norm, torch.zeros_like(norm)], -1)


def modulus(z):
    z_mod = z.norm(p=2, dim=-1)

    # if z.requires_grad:
    #     # z_mod_mask.register_hook(HookDetectNan("z_mod_mask in modulus"))
    #     z_mod.register_hook(HookDetectNan("z_mod in modulus"))
    #     z.register_hook(HookDetectNan("z in modulus"))

    return z_mod


def fft(input, direction='C2C', inverse=False):
    """
        Interface with torch FFT routines for 2D signals.
        Example
        -------
        x = torch.randn(128, 32, 32, 2)
        x_fft = fft(x, inverse=True)
        Parameters
        ----------
        input : tensor
            complex input for the FFT
        direction : string
            'C2R' for complex to real, 'C2C' for complex to complex
        inverse : bool
            True for computing the inverse FFT.
            NB : if direction is equal to 'C2R', then the transform
            is automatically inverse.
    """
    if direction == 'C2R':
        inverse = True

    if not iscomplex(input):
        raise(TypeError('The input should be complex (e.g. last dimension is 2)'))

    if (not input.is_contiguous()):
        raise (RuntimeError('Tensors must be contiguous!'))

    if direction == 'C2R':
        output = torch.irfft(input, 2, normalized=False, onesided=False)*input.size(-2)*input.size(-3)
    elif direction == 'C2C':
        if inverse:
            output = torch.ifft(input, 2, normalized=False)*input.size(-2)*input.size(-3)
        else:
            output = torch.fft(input, 2, normalized=False)

    return output



class PhaseHarmonics2(Function):
    @staticmethod
    def forward(ctx, z, k):
        z = z.detach()
        x, y = real(z), imag(z)
        r = z.norm(p=2, dim=-1)
        theta = torch.atan2(y, x)
        ktheta = k * theta
        eiktheta = torch.stack((torch.cos(ktheta), torch.sin(ktheta)), dim=-1)
        ctx.save_for_backward(x, y, r, k)
        return r.unsqueeze(-1)*eiktheta

    @staticmethod
    def backward(ctx, grad_output):
        x, y, r, k = ctx.saved_tensors
        theta = torch.atan2(y, x)
        ktheta = k * theta
        cosktheta = torch.cos(ktheta)
        sinktheta = torch.sin(ktheta)
        costheta = torch.cos(theta)
        sintheta = torch.sin(theta)

        df1dx = costheta*cosktheta + k*sintheta*sinktheta
        df2dx = costheta*sinktheta - k*sintheta*cosktheta
        df1dy = sintheta*cosktheta - k*costheta*sinktheta
        df2dy = sintheta*sinktheta + k*costheta*cosktheta

        dx1 = df1dx*grad_output[...,0] + df2dx*grad_output[...,1]
        dx2 = df1dy*grad_output[...,0] + df2dy*grad_output[...,1]

        return torch.stack((dx1, dx2), -1), k # dummy gradient torch.zeros_like(k)

class PhaseHarmonicsIso(Function):
    # z.size(): (J,L2,M,N,1,2)
    # k.size(): (K)
    @staticmethod
    def forward(ctx, z, k):
        z = z.detach()
        x, y = real(z), imag(z)
        r = z.norm(p=2, dim=-1)  # (J, L2, M, N, 1)
        theta = torch.atan2(y, x)  # (J, L2, M, N, 1)
        ktheta = k * theta  # (J, L2, M, N, K)
        eiktheta = torch.stack((torch.cos(ktheta), torch.sin(ktheta)), dim=-1)
        # eiktheta.size(): (J, L2, M, N, K, 2)
        ctx.save_for_backward(x, y, r, k)
        return r.unsqueeze(-1)*eiktheta

    @staticmethod
    def backward(ctx, grad_output):
        x, y, r, k = ctx.saved_tensors
        theta = torch.atan2(y, x)
        ktheta = k * theta
        cosktheta = torch.cos(ktheta)
        sinktheta = torch.sin(ktheta)
        costheta = torch.cos(theta)
        sintheta = torch.sin(theta)

        df1dx = costheta*cosktheta + k*sintheta*sinktheta
        df2dx = costheta*sinktheta - k*sintheta*cosktheta
        df1dy = sintheta*cosktheta - k*costheta*sinktheta
        df2dy = sintheta*sinktheta + k*costheta*cosktheta

        dx1 = df1dx*grad_output[..., 0] + df2dx*grad_output[..., 1]
        dx2 = df1dy*grad_output[..., 0] + df2dy*grad_output[..., 1]

        return torch.stack((dx1, dx2), -1), torch.zeros_like(k)

#phase_exp = PhaseHarmonics2.apply

# rest

# k in Linear or Log
class PhaseExpLL(nn.Module):
    def __init__(self, K, k_type='linear', keep_k_dim=False, check_for_nan=False):
        super(PhaseExpLL, self).__init__()
        self.K = K
        self.keep_k_dim = keep_k_dim
        self.check_for_nan = check_for_nan
        assert k_type in ['linear', 'log2']
        self.k_type = k_type

    def forward(self, z):
        s = z.size()
        z_mod = z.norm(p=2, dim=-1)  # modulus

        eitheta = phaseexp(z, z_mod.unsqueeze(-1))  # phase

        # compute phase exponent : |z| * exp(i k theta)
        if self.k_type == 'linear':
            eiktheta = pows(eitheta, self.K - 1, dim=1)
        elif self.k_type == 'log2':
            eiktheta = log2_pows(eitheta, self.K - 1, dim=1)
        z_pe = z_mod.unsqueeze(-1) * eiktheta

        if not self.keep_k_dim:
            z_pe = z_pe.view(s[0], -1, *s[2:])

#        if z.requires_grad and self.check_for_nan:
#            z.register_hook(HookDetectNan("z in PhaseExp"))
#            if self.K > 1:
#                z_mod.register_hook(HookDetectNan("z_mod in PhaseExp"))
#            eitheta.register_hook(HookDetectNan("eitheta in PhaseExp"))
#            eiktheta.register_hook(HookDetectNan("eiktheta in PhaseExp"))
#            z_pe.register_hook(HookDetectNan("z_pe in PhaseExp"))

        return z_pe

# comptue only a single phase exponent k, k >= 0 integer
class PhaseExpSk(nn.Module):
    def __init__(self, keep_k_dim=False, check_for_nan=False):
        super(PhaseExpSk, self).__init__()
        self.keep_k_dim = keep_k_dim
        self.check_for_nan = check_for_nan

    def forward(self, z, k):
        s = z.size()
        z_mod = z.norm(p=2, dim=-1)  # modulus

        eitheta = phaseexp(z, z_mod.unsqueeze(-1))  # phase

        if k == 0:
            eiktheta = [ones_like(z)]
        elif k == 1:
            eiktheta = eitheta
        elif k > 1:
            eitheta_acc = eitheta
            for kb in range(2,k+1):
                eitheta_acc = mul(eitheta_acc,eitheta)
            eiktheta = eitheta_acc
        else:
            assert k>=0, 'need postive k exponent'

        z_pe = z_mod.unsqueeze(-1) * eiktheta

        if not self.keep_k_dim:
            z_pe = z_pe.view(s[0], -1, *s[2:])

    #        if z.requires_grad and self.check_for_nan:
#            z.register_hook(HookDetectNan("z in PhaseExp"))
#            if self.K > 1:
#                z_mod.register_hook(HookDetectNan("z_mod in PhaseExp"))
#            eitheta.register_hook(HookDetectNan("eitheta in PhaseExp"))
#            eiktheta.register_hook(HookDetectNan("eiktheta in PhaseExp"))
#            z_pe.register_hook(HookDetectNan("z_pe in PhaseExp"))
        return z_pe

class PhaseExp_par(nn.Module):
    def __init__(self, K, k_type='log2', keep_k_dim=False, check_for_nan=False):
        super(PhaseExp_par, self).__init__()
        self.K = K
        self.keep_k_dim = keep_k_dim
        self.check_for_nan = check_for_nan
        assert k_type in ['linear', 'log2']
        self.k_type = k_type

    def forward(self, z):
        s = z.size()
        z_mod = z.norm(p=2, dim=-1)  # modulus

        eitheta = phaseexp(z, z_mod.unsqueeze(-1))  # phase
        #print(eitheta.size())

        # compute phase exponent : |z| * exp(i k theta)
        if self.k_type == 'linear':
            eiktheta = pows(eitheta, self.K - 1, dim=1)
            #print(eiktheta.size())
        elif self.k_type == 'log2':
            eiktheta = log2_pows(eitheta, self.K - 1, dim=1)
        #print(z_mod.unsqueeze(1).size())
        z_pe = z_mod.unsqueeze(1).unsqueeze(-1) * eiktheta

        if not self.keep_k_dim:
            z_pe = z_pe.view(s[0], -1, *s[2:])

#        if z.requires_grad and self.check_for_nan:
#            z.register_hook(HookDetectNan("z in PhaseExp"))
#            if self.K > 1:
#                z_mod.register_hook(HookDetectNan("z_mod in PhaseExp"))
#            eitheta.register_hook(HookDetectNan("eitheta in PhaseExp"))
#            eiktheta.register_hook(HookDetectNan("eiktheta in PhaseExp"))
#            z_pe.register_hook(HookDetectNan("z_pe in PhaseExp"))

        return z_pe



class StablePhase(Function):
    @staticmethod
    def forward(ctx, z):
        z = z.detach()
        x, y = real(z), imag(z)
        r = z.norm(p=2, dim=-1)

        # NaN positions
        eps = 1e-32
        mask_real_neg = (torch.abs(y) <= eps) * (x <= 0)
        mask_zero = r <= eps

        x_tilde = r + x
        # theta = torch.atan(y / x_tilde) * 2
        theta = torch.atan2(y, x)

        # relace NaNs
        theta.masked_fill_(mask_real_neg, np.pi)
        theta.masked_fill_(mask_zero, 0.)

        # ctx.save_for_backward(x.detach(), y.detach(), r.detach())
        ctx.save_for_backward(x, y, r, mask_real_neg, mask_zero)
        return theta

    @staticmethod
    def backward(ctx, grad_output):
        x, y, r, mask_real_neg, mask_zero = ctx.saved_tensors

        # some intermediate variables
        x_tilde = r + x
        e = x_tilde ** 2 + y ** 2

        # derivative with respect to the real part
        dtdx = - y * x_tilde * 2 / (r * e)
        mask_real_neg_bis = (torch.abs(y) == 0) * (x <= 0)
        dtdx.masked_fill_(mask_real_neg, 0)
        dtdx.masked_fill_(mask_zero, 0)

        # derivative with respect to the imaginary part
        dtdy = x * x_tilde * 2 / (r * e)
        dtdy[mask_real_neg] = -1 / r[mask_real_neg]
        # dtdy.masked_fill_(mask, 0)
        dtdy.masked_fill_(mask_zero, 0)

        dtdz = grad_output.unsqueeze(-1) * torch.stack((dtdx, dtdy), dim=-1)
        return dtdz

phase = StablePhase.apply


class PhaseHarmonic(nn.Module):
    def __init__(self, check_for_nan=False):
        super(PhaseHarmonic, self).__init__()
        self.check_for_nan = check_for_nan

    def forward(self, z, k):
        # check type ok k, move to float
        #if not is_long_tensor(k):
        #    raise TypeError("Expected torch(.cuda).LongTensor but got {}".format(k.type()))
        #if is_double_tensor(z):
        #    k = k.double()
        #else:
        #    k = k.float()

        #s = z.size()

        #print('z shape',z.shape,z.size())
        z_mod = modulus(z)  # modulus
        #print('z_mod shape',z_mod.shape)

        # compute phase

        theta = phase(z)  # phase
        #print('theta shape',theta.shape)
        #k = k.unsqueeze(0) #.unsqueeze(1)
        #print('k shape',k.shape)
        #k = k.unsqueeze(-1).unsqueeze(-1)
        #print('k shape',k.shape)
        ktheta = k * theta

        eiktheta = torch.stack((torch.cos(ktheta), torch.sin(ktheta)), dim=-1)

        # compute phase exponent : |z| * exp(i k theta)
        z_pe = z_mod.unsqueeze(-1) * eiktheta

        if z.requires_grad and self.check_for_nan:
            z.register_hook(HookDetectNan("z in PhaseExp"))
                # , torch.stack((z_mod, z_mod), dim=-1), torch.stack((theta, theta), dim=-1)))
            z_mod.register_hook(HookDetectNan("z_mod in PhaseExp"))
            eiktheta.register_hook(HookDetectNan("eiktheta in PhaseExp"))
            z_pe.register_hook(HookDetectNan("z_pe in PhaseExp"))

        return z_pe


class StablePhaseExp(Function):
    @staticmethod
    def forward(ctx, z, r):
        eitheta = z / r
        eitheta.masked_fill_(r == 0, 0)

        ctx.save_for_backward(eitheta, r)
        return eitheta

    @staticmethod
    def backward(ctx, grad_output):
        eitheta, r = ctx.saved_tensors

        dldz = grad_output / r
        dldz.masked_fill_(r == 0, 0)

        dldr = - eitheta * grad_output / r
        dldr.masked_fill_(r == 0, 0)
        dldr = dldr.sum(dim=-1).unsqueeze(-1)

        return dldz, dldr


phaseexp = StablePhaseExp.apply


# periodic shift in 2d

class PeriodicShift2D(nn.Module):
    def __init__(self, M,N,shift1,shift2):
        super(PeriodicShift2D, self).__init__()
        self.M = M
        self.N = N
        self.shift1 = shift1 % M # [0,M-1]
        self.shift2 = shift2 % N # [0,N-1]

    def forward(self, input):
        # input dim is (1,P_c,M,N,2)
        # per. shift along M and N dim by shift1 and shift2
        #M = input.shape[2]
        #N = input.shape[3]
        M = self.M
        N = self.N
        shift1 = self.shift1
        shift2 = self.shift2

        #blk11 = [[0,0],[shift1-1,shift2-1]]
        #blk22 = [[shift1,shift2],[M-1,N-1]]
        #blk12 = [[shift1,0],[M-1,shift2-1]]
        #blk21 = [[0,shift2],[shift1-1,N-1]]
        output = input.clone()
        output[:,:,0:M-shift1,0:N-shift2,:] = input[:,:,shift1:M,shift2:N,:]
        output[:,:,0:M-shift1,N-shift2:N,:] = input[:,:,shift1:M,0:shift2,:]
        output[:,:,M-shift1:M,0:N-shift2,:] = input[:,:,0:shift1,shift2:N,:]
        output[:,:,M-shift1:M,N-shift2:N,:] = input[:,:,0:shift1,0:shift2,:]

        return output


def complex_mul(a, b):
    ar = a[..., 0]
    br = b[..., 0]
    ai = a[..., 1]
    bi = b[..., 1]
    real = ar*br - ai*bi
    imag = ar*bi + ai*br

    return torch.stack((real, imag), dim=-1)


def shift2(im_, u):
    # u: (1, P_c ,m, 2)
    # im_: (1, P_c, N, N)
    size = im_.size(-2)
    u = u.type(torch.float).cuda()
    im = torch.stack((im_, torch.zeros(im_.size()).type(torch.cuda.FloatTensor)), dim=-1)  # (1, P_c, N, N, 2)
    im_fft = torch.fft(im, 2)  # (1, P_c, N, N, 2)
    map = torch.arange(size).repeat(tuple(u.size()[:3])+(1,)).type(torch.float).cuda()  # (1, P_c, m, N)
    z = torch.matmul(map.unsqueeze(-1), u.unsqueeze(-2)).type(torch.float).cuda()  # (1, P_c, m, N, 1), (1, P_c, m, 1, 2)->(1, P_c, m, N, 2)
    sp = z[..., 0].unsqueeze(-1).repeat(1,1,1,1,size) + z[..., 1].unsqueeze(-2).repeat(1,1,1,size,1)  # (1, P_c, m, N, N)
    # compute e^(-iw.u0)
    fft_shift = torch.stack((torch.cos(2*np.pi*sp/size), torch.sin(2*np.pi*sp/size)), dim=-1)  # (1, P_c, m, N, N, 2)
    im_shift_fft = complex_mul(fft_shift, im_fft.unsqueeze(2).repeat(1,1,u.size(2), 1, 1, 1))  # (1, P_c, m, N, N, 2)
    im_shift = torch.ifft(im_shift_fft, 2)  # (1, P_c, m, N, N, 2)
#    plt.imshow(im_shift[0, ..., 0]); plt.show()
    return im_shift


def unshift2(ims, u):
    size = ims.size(-2)
    u = u.type(torch.float)
    ims_fft = torch.fft(ims, 2)  # (1, P_c, m, N, N, 2)
    map = torch.arange(size).repeat(tuple(u.size()[:3])+(1,)).type(torch.cuda.FloatTensor)  # (1, P_c, m, N)
    u_back = -u  # (1, P_c, m, 2)
    z = torch.matmul(map.unsqueeze(-1), u_back.unsqueeze(-2))  # (1, P_c, m, N, 1), (1, P_c, m, 1, 2)->(1, P_c, m, N, 2)
    sp = z[..., 0].unsqueeze(-1).repeat(1,1,1,1,size) + z[..., 1].unsqueeze(-2).repeat(1,1,1,size,1)  # (1, P_c, m, N, N)
    # compute e^(-iw.u0)
    fft_shift = torch.stack((torch.cos(2*np.pi*sp/size), torch.sin(2*np.pi*sp/size)), dim=-1)  # (1, P_c, m, N, N, 2)
    im_shift_fft = complex_mul(ims_fft, fft_shift)  # (1, P_c, m, N, N, 2)
    im_shift = torch.ifft(im_shift_fft, 2)[..., 0]  # (1, P_c, m, N, N)
    return im_shift

def indices_unpad(im, indices, pad):
    size = im.size(-2)
    indices = indices.view(tuple(im.size()[:2])+(-1,))
    indices_col = (indices % (size+2*pad)).type(torch.LongTensor).cuda()
    indices_col = (indices_col - pad)%size
    indices_row = ((indices )//(size+2*pad)).type(torch.LongTensor).cuda()
    indices_row = (indices_row - pad)%size

    indices = size*indices_row + indices_col
    return indices, indices_row, indices_col


def local_max_indices(im):
    #  im: (1, P_c, N, N)
    size = im.size(-1)
    mp1 = torch.nn.MaxPool2d(5, stride=torch.Size([1,1]), return_indices=True)
    mp2 = torch.nn.MaxPool2d(7, stride=torch.Size([1,1]), return_indices=True)

    im_pad1 = F.pad(im, (2, 2, 2, 2), mode='circular')
    im_pad2 = F.pad(im, (3, 3, 3, 3), mode='circular')

    maxed1 = mp1(im_pad1)[1]
    maxed2 = mp2(im_pad2)[1]

    maxed1 = indices_unpad(im, maxed1, 2)
    maxed2 = indices_unpad(im, maxed2, 3)
    eq = torch.eq(maxed1[0], maxed2[0]).type(torch.cuda.FloatTensor)  # (1, P_c, N*N)
    del(maxed2)
    T = (-(1 - eq) + eq * maxed1[0]).type(torch.cuda.FloatTensor)  # (1, P_c, N*N)
    del(maxed1)
    del(eq)
    maxima = torch.max(T, dim=-1)[0].unsqueeze(-1).repeat(1, 1, T.size(-1))  # (1, P_c, N*N)
    threshold = torch.gt(T,  maxima/5).type(torch.float)
    T = T*threshold - (1-threshold)  # (1, P_c, N*N)

    which = torch.arange(im.size(1)).unsqueeze(0).unsqueeze(-1).repeat(1, 1, size*size).type(torch.cuda.FloatTensor)  # (1, P_c, N*N)
    T = torch.stack((T, which), dim=1)  # (1, 2, P_c, N*N)
    del(which)
    T = T.view(2, -1)  # (2, P_c*N*N)
    T = T.unique(sorted=False, dim=-1)  # (2, m_sum)
    T = T.unsqueeze(0).repeat(im.size(1), 1, 1)  # (P_c, 2, m_sum)
    z = torch.arange(im.size(1)).unsqueeze(1).repeat(1, T.size(-1)).type(torch.cuda.FloatTensor)  # (P_c, m_sum)
    affect = torch.eq(T[:, 1, :], z).type(torch.cuda.FloatTensor)  # (P_c, m_sum)
    del(z)
    T = (T[:, 0, :]*affect + 3*size*size*(1-affect)).unsqueeze(0)  # (1, P_c, m_sum)
    del(affect)
    T = T.sort()[0]
    U = T - 3*size*size
    U_z = (U == 0).type(torch.cuda.FloatTensor)
    U_z = U_z.sum(dim=-1)
    U_min = int(U_z.min())

    T = T[:, :, :-U_min]  # (1, P_c, m_sum)

    return T, torch.stack((T//size, T%size), dim=-1)


def orth_phase(im2, loc):
    #im2 (1, P_c, N, N, 2)
    phase = torch.atan2(im2[...,1], im2[...,0])  # (1, P_c, N, N)
    shifted_phase = shift2(phase, loc)  # (1, P_c, m, N, N, 2)
    t_s_phase = torch.transpose(shifted_phase, -2, -3)  # (1, P_c, m, N, N, 2)
    t_s_phase = torch.flip(t_s_phase.unsqueeze(-3), [-3,-4]).squeeze()  # (1, P_c, m, N, N, 2)
    orth_ph = unshift2(t_s_phase, loc)  # (1, P_c, m, N, N)
    return phase, orth_ph


def shifted_phase(im2, loc, theta):
    """
    theta: tensor of size (P_c)
    """
    size = im2.size(-2)
    theta_ = theta.unsqueeze(0).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)
    theta_ = theta_.repeat(1, 1, loc.size(2), size, size)  # (1, P_c, m, N, N)
    phase, orth_ph = orth_phase(im2, loc)  # (1, P_c, N, N), (1, P_c, m, N, N)
    return torch.cos(theta_)*phase.unsqueeze(2).repeat(1, 1, loc.size(2), 1, 1) - torch.sin(theta_)*orth_ph  # (1, P_c, m, N, N)



def periodic_distance(x, y, N):
    return torch.min(torch.abs(x-y), torch.abs(x-y+N)).min(torch.abs(x-y-N))


def dist_to_max(u, size):
    z = torch.arange(size).repeat(tuple(u.size()[:3])+(1,)).type(torch.cuda.FloatTensor)  # (1, P_c, m, N)
    z1 = periodic_distance(z, u[..., 0].unsqueeze(3).repeat(1, 1, 1, size), size).unsqueeze(-1).repeat(1, 1, 1, 1, size)  # (1, P_c, m, N, N)
    z2 = periodic_distance(z, u[..., 1].unsqueeze(3).repeat(1, 1, 1, size), size).unsqueeze(-2).repeat(1, 1, 1, size, 1)  # (1, P_c, m, N, N)
    z1 = z1**2
    z2 = z2**2
    z = torch.sqrt(z1 + z2)  # (1, P_c, m, N, N)
    z_min = torch.min(z, dim=2)[0].unsqueeze(2).repeat(1, 1, u.size(2), 1, 1)  # (1, P_c, m, N, N)
    return torch.eq(z, z_min).type(torch.float).cuda()

def new_phase(im1, im2, theta):
    # im1 and im2 (1, P_c, N, N, 2)
    im = im1.norm(p=2, dim=-1)*im2.norm(p=2, dim=-1)  # (1, P_c, N, N)
    loc = local_max_indices(im)[1]  # (1, P_c, m, 2)
    shifted_ph = shifted_phase(im2, loc, theta)  # (1, P_c, m, N, N)
    vor = dist_to_max(loc, im1.size(-2)).type(torch.float).cuda()  # (1, P_c, m, N, N)
    return (shifted_ph*vor).sum(dim=2)


def phase_rot(im1, im2, theta):
    z = im2.norm(p=2, dim=-1)
    ph_rot = new_phase(im1, im2, theta)
    return torch.stack((z*torch.cos(ph_rot), z*torch.sin(ph_rot)), dim=-1)
